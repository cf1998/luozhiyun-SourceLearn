# 10.深入k8s：调度的优先级及抢占机制源码分析

> 转载请声明出处哦~，本篇文章发布于luozhiyun的博客：https://www.luozhiyun.com
>
> 源码版本是[1.19](https://github.com/kubernetes/kubernetes/tree/release-1.19)



上一篇我们将了获取node成功的情况，如果是一个优先pod获取node失败，那么就会进入到抢占环节中。

## 调度的优先级与抢占机制

正常情况下，当一个 Pod 调度失败后，它就会被暂时“搁置”起来，直到 Pod 被更新，或者集群状态发生变化，调度器才会对这个 Pod 进行重新调度。但是我们可以通过PriorityClass优先级来避免这种情况。通过设置优先级一些优先级比较高的pod，如果pod 调度失败，那么并不会被”搁置”，而是会”挤走”某个 node 上的一些低优先级的 pod，这样就可以保证高优先级的 pod 调度成功。

要使用PriorityClass，首先我们要定义一个PriorityClass对象，例如：

```yaml
apiVersion: v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
```

value越高则优先级越高；globalDefault 被设置为 true 的话，那就意味着这个 PriorityClass 的值会成为系统的默认值，如果是false则表示我们只希望声明使用该 PriorityClass 的 Pod 拥有值为 1000000 的优先级，而对于没有声明 PriorityClass 的 Pod 来说，它们的优先级就是 0。

Pod 就可以声明使用它了：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  priorityClassName: high-priority
```

高优先级的 Pod 调度失败的时候，调度器的抢占能力就会被触发。调度器就会试图从当前集群里寻找一个节点，使得当这个节点上的一个或者多个低优先级 Pod 被删除后，待调度的高优先级 Pod 就可以被调度到这个节点上。

高优先级Pod进行抢占的时候会将pod的 nominatedNodeName 字段，设置为被抢占的 Node 的名字。然后，在下一周期中决定是不是要运行在被抢占的节点上，当这个Pod在等待的时候，如果有其他更高优先级的 Pod 也要抢占同一个节点，那么调度器就会清空原抢占者的 spec.nominatedNodeName 字段，从而允许更高优先级的抢占者执行抢占。

## 源码解析

这里我依旧拿出这张图来进行讲解，上一篇我们将了获取node成功的情况，如果是一个优先pod获取node失败，那么就会进入到抢占环节中。

![调度流程](10.深入k8s：优先级及抢占机制源码分析/调度流程.png)

通过上一篇的分析，我们知道，在scheduleOne方法中执行sched.Algorithm.Schedule会选择一个合适的node节点，如果获取node失败，那么就会进入到一个if逻辑中执行抢占。

代码路径：pkg/scheduler/scheduler.go

```go
func (sched *Scheduler) scheduleOne(ctx context.Context) {
	...
	//为pod资源对象选择一个合适的节点
	scheduleResult, err := sched.Algorithm.Schedule(schedulingCycleCtx, prof, state, pod)
	//获取node失败，抢占逻辑
	if err != nil { 
		//上面调用失败之后，下面会根据pod执行抢占
		nominatedNode := ""
		if fitError, ok := err.(*core.FitError); ok {
			if !prof.HasPostFilterPlugins() {
				klog.V(3).Infof("No PostFilter plugins are registered, so no preemption will be performed.")
			} else { 
				result, status := prof.RunPostFilterPlugins(ctx, state, pod, fitError.FilteredNodesStatuses)
				if status.Code() == framework.Error {
					klog.Errorf("Status after running PostFilter plugins for pod %v/%v: %v", pod.Namespace, pod.Name, status)
				} else {
					klog.V(5).Infof("Status after running PostFilter plugins for pod %v/%v: %v", pod.Namespace, pod.Name, status)
				}
				//抢占成功后，将nominatedNodeName设置为被抢占的 Node 的名字，然后重新进入下一个调度周期
				if status.IsSuccess() && result != nil {
					nominatedNode = result.NominatedNodeName
				}
			} 
			metrics.PodUnschedulable(prof.Name, metrics.SinceInSeconds(start))
		} else if err == core.ErrNoNodesAvailable { 
			metrics.PodUnschedulable(prof.Name, metrics.SinceInSeconds(start))
		} else {
			klog.ErrorS(err, "Error selecting node for pod", "pod", klog.KObj(pod))
			metrics.PodScheduleError(prof.Name, metrics.SinceInSeconds(start))
		}
		sched.recordSchedulingFailure(prof, podInfo, err, v1.PodReasonUnschedulable, nominatedNode)
		return
	}
	...
}
```

在这个方法里面RunPostFilterPlugins会执行具体的抢占逻辑，然后返回被抢占的node节点。抢占者并不会立刻被调度到被抢占的 node 上，调度器只会将抢占者的 status.nominatedNodeName 字段设置为被抢占的 node 的名字。然后，抢占者会重新进入下一个调度周期，在新的调度周期里来决定是不是要运行在被抢占的节点上，当然，即使在下一个调度周期，调度器也不会保证抢占者一定会运行在被抢占的节点上。

这样设计的一个重要原因是调度器只会通过标准的 DELETE API 来删除被抢占的 pod，所以，这些 pod 必然是有一定的“优雅退出”时间（默认是 30s）的。而在这段时间里，其他的节点也是有可能变成可调度的，或者直接有新的节点被添加到这个集群中来。

而在抢占者等待被调度的过程中，如果有其他更高优先级的 pod 也要抢占同一个节点，那么调度器就会清空原抢占者的 status.nominatedNodeName 字段，从而允许更高优先级的抢占者执行抢占，并且，这也使得原抢占者本身也有机会去重新抢占其他节点。



接着我们继续看，RunPostFilterPlugins会遍历所有的postFilterPlugins，然后执行runPostFilterPlugin方法：

```go
func (f *frameworkImpl) RunPostFilterPlugins(ctx context.Context, state *framework.CycleState, pod *v1.Pod, filteredNodeStatusMap framework.NodeToStatusMap) (_ *framework.PostFilterResult, status *framework.Status) {
	startTime := time.Now()
	defer func() {
		metrics.FrameworkExtensionPointDuration.WithLabelValues(postFilter, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime))
	}()

	statuses := make(framework.PluginToStatus)
	//postFilterPlugins里面只有一个defaultpreemption
	for _, pl := range f.postFilterPlugins {
		r, s := f.runPostFilterPlugin(ctx, pl, state, pod, filteredNodeStatusMap)
		if s.IsSuccess() {
			return r, s
		} else if !s.IsUnschedulable() {
			// Any status other than Success or Unschedulable is Error.
			return nil, framework.NewStatus(framework.Error, s.Message())
		}
		statuses[pl.Name()] = s
	}

	return nil, statuses.Merge()
}
```

根据我们上一节看的scheduler的初始化可以知道设置的PostFilter如下：

代码路径：pkg/scheduler/algorithmprovider/registry.go

```
	PostFilter: &schedulerapi.PluginSet{
			Enabled: []schedulerapi.Plugin{
				{Name: defaultpreemption.Name},
			},
		},
```

可见，目前只有一个defaultpreemption来执行抢占逻辑，在postFilterPlugins循环里面会调用到runPostFilterPlugin然后运行defaultpreemption的PostFilter方法，最后执行到preempt执行具体抢占逻辑。

代码路径：pkg/scheduler/framework/plugins/defaultpreemption/default_preemption.go

```go
func (pl *DefaultPreemption) PostFilter(...) (*framework.PostFilterResult, *framework.Status) {
	...
	//执行抢占
	nnn, err := pl.preempt(ctx, state, pod, m)
	...
	return &framework.PostFilterResult{NominatedNodeName: nnn}, framework.NewStatus(framework.Success)
}
```





## Reference

https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/

https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/

https://kubernetes.io/docs/concepts/configuration/pod-overhead/